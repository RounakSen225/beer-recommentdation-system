# -*- coding: utf-8 -*-
"""Copy of CSE258.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GvLTxaIl7FhNk7294xKuaJY497qN56aq

# Part 1: Exploratory Data Analysis

DataSource

    https://datarepo.eng.ucsd.edu/mcauley_group/data/beer/beeradvocate.json.gz

Important Attributes

    beer/ABV (numerical): alcohol by volume
    beer/beerID (numerical): beer ID
    beer/brewerID (numerical): brewer ID
    beer/name (categorical): name of beer
    beer/style (categorical): type/style of beer
    review/overall (numerical): overall beer rating out of 5
    review/appearance (numerical): beer rating based on appearance out of 5
    review/aroma (numerical): beer rating based on aroma out of 5
    review/palate (numerical): beer rating based on palate out of 5
    review/taste (numerical): beer rating based on taste out of 5
    review/text (text): beer review text
    review/time (UNIX time): date of review entry


Questions

    1. Will we use the entire dataset? If not, why?
    2. Why do we need sampling and how are we implementing it?
    3. What are the most popular styles of beer?
    4. What are the different types or classifications of beer available?
    5. Which beer has the best overall rating?
    6. The number of beers produced by each brewer
    7. What is the relationship between the beer's characteristics (appearance, aroma, palate, taste) and its overall rating?
    8. Sentiment analysis of beer reviews: How many are positive, negative, or neutral?
    9. Is there a correlation between the length or sentiment of the review text and the beer's overall rating?
    10. Can we predict the overall rating of a beer based on its characteristics and review?
    11. Temporal trends: How do beer reviews and ratings distribute over time?
"""

import numpy as np
import gzip
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model
from collections import defaultdict
import seaborn as sns
import random

# from google.colab import drive
# drive.mount('/content/drive')

drive.mount("/content/drive", force_remount=True)

!pip install gdown

"""## 1. Will we use the entire dataset? If not, why?"""

!wget 'https://datarepo.eng.ucsd.edu/mcauley_group/data/beer/beeradvocate.json.gz'

z = gzip.open("beeradvocate.json.gz", "rt", encoding='utf8')
beeradvocate_dataset = []
for l in z:
    beeradvocate_dataset.append(eval(l))

df = pd.DataFrame(beeradvocate_dataset)
df = df.sort_index(axis=1)

df.shape

"""As we can see that size of the data is roughly 1500k, processing and training models on a dataset of such size extensive computational resources and leads to prolonged training times. To mitigate these challenges, we plan to downsize the dataset to 100,000 samples through sampling techniques provides a more practical approach. This reduction accelerates model development, conserves computational resources, and streamlines the workflow while maintaining essential data patterns.

## 2. Why do we need sampling and how are we implementing it?

Given the original dataset's massive size of 1,586,615 samples, training machine learning models on it would be computationally intensive and time-consuming. Therefore, we're implementing sampling to create a more manageable subset of the data.

In particular, we aim to maintain a balanced dataset, where each class or category is represented adequately. It ensures that for each group or category within the dataset, we sample a maximum of 50,000 samples or fewer if the group contains fewer than 50,000 samples. This approach helps us strike a balance between reducing the dataset's size, managing computational resources, and preserving class balance, ultimately enabling more efficient model development and training.
"""

df['review/overall'] = pd.to_numeric(df['review/overall'], errors='coerce')
df['review/appearance'] = pd.to_numeric(df['review/appearance'], errors='coerce')
df['review/aroma'] = pd.to_numeric(df['review/aroma'], errors='coerce')
df['review/palate'] = pd.to_numeric(df['review/palate'], errors='coerce')
df['review/taste'] = pd.to_numeric(df['review/taste'], errors='coerce')
df['beer/ABV'] = pd.to_numeric(df['beer/ABV'], errors='coerce')

# Handle any NaN values that might have been introduced by conversion
# You can choose to fill them with a default value or drop them
df = df.dropna(subset=['review/overall'])
df = df.dropna(subset=['review/appearance'])
df = df.dropna(subset=['review/aroma'])
df = df.dropna(subset=['review/palate'])
df = df.dropna(subset=['review/taste'])
df = df.dropna(subset=['beer/ABV'])
df = df.dropna(subset=['review/text'])


df['timestamp'] = pd.to_datetime(df['review/time'] , unit='s')
df['mon'] = df['timestamp'].dt.month
df['wday'] = df['timestamp'].dt.weekday  # Monday=0, Sunday=6
df['hour'] = df['timestamp'].dt.hour

# Convert 'review/overall' into a binary classification
df['binary_overall'] = (df['review/overall'] >= 4).astype(int)

def parseData(fname):
    for l in open(fname):
        yield eval(l)

data = list(parseData("/content/drive/MyDrive/CSE 258-Assignment2/data100k_new.json"))[0]

df = pd.DataFrame(data[0])
df = df.sort_index(axis=1)

print(df.shape)

df['review/profileName'] = df['review\\/profileName']
df['review/overall'] = df['review\\/overall']
df['review/appearance'] = df['review\\/appearance']
df['review/aroma'] = df['review\\/aroma']
df['review/palate'] = df['review\\/palate']
df['review/taste'] = df['review\\/taste']
df['beer/ABV'] = df['beer\\/ABV']
df['review/text'] = df['review\\/text']
df['review/time'] = df['review\\/time']
df['beer/beerId'] = df['beer\\/beerId']
df['beer/brewerId'] = df['beer\\/brewerId']
df['beer/name'] = df['beer\\/name']
df['beer/style'] = df['beer\\/style']

df = df.drop(columns=['review\\/profileName'])
df = df.drop(columns=['review\\/overall'])
df = df.drop(columns=['review\\/appearance'])
df = df.drop(columns=['review\\/aroma'])
df = df.drop(columns=['review\\/palate'])
df = df.drop(columns=['review\\/taste'])
df = df.drop(columns=['beer\\/ABV'])
df = df.drop(columns=['review\\/text'])
df = df.drop(columns=['review\\/time'])
df = df.drop(columns=['beer\\/beerId'])
df = df.drop(columns=['beer\\/brewerId'])
df = df.drop(columns=['beer\\/name'])
df = df.drop(columns=['beer\\/style'])

grouped = df.groupby('binary_overall', group_keys=False)

# Sample 25,000 records from each class
# Make sure each class has at least 25,000 records
df = grouped.apply(lambda x: x.sample(n=min(len(x), 50000)))

# Check the balance of the classes
print(df['binary_overall'].value_counts())

# Histogram for Positive and Negative Overall Beer Ratings
condition = df['review/overall'] >= 4

# Calculate the number of entries satisfying the condition
count_positive = df[condition].shape[0]
count_negative = len(df) - count_positive

# Categories
categories = ['Negative (overall_rating < 4)', 'Positive (overall_rating >= 4)']

# Bar width
bar_width = 0.25

# Set the positions for the bars on X-axis
r1 = range(len(categories))
r1 = [x + bar_width/2 for x in r1]

# Plotting the bars
plt.bar(r1, [count_negative, count_positive], width=bar_width, edgecolor='grey', label='Negative Count')

plt.xlabel('Categories')
plt.ylabel('Count')
plt.title('Bar Plot of Counts for Positive and Negative Reviews')
plt.xticks([r + bar_width/2 for r in range(len(categories))], categories)

plt.show()

df = df.sort_index(axis=1)

df.head().transpose()

cols = ['beer/ABV', 'beer/beerId', 'beer/brewerId', 'beer/name', 'beer/style',
        'review/overall', 'review/aroma', 'review/palate', 'review/taste',
        'review/appearance']
df[cols].describe(include='all').transpose()

print(f"Total number of entries in the dataset {df.shape[0]}")
print(f"Number of Unique Beer IDs {df['beer/beerId'].nunique()}")
print(f"Number of Unique Brewer IDs {df['beer/brewerId'].nunique()}")
print(f"Number of Unique Beer names {df['beer/name'].nunique()}")
print(f"Number of Unique Beer styles {df['beer/style'].nunique()}")

"""## 3. What are the most popular styles of beer?"""

# Checking the beer styles with highest overall ratings:
reviews = pd.DataFrame(df.groupby('beer/style')[['review/overall', 'review/aroma', 'review/palate', 'review/taste', 'review/appearance']].mean())
reviews['count_reviews'] = pd.DataFrame(df.groupby('beer/style')['review/overall'].count())
reviews.sort_values('count_reviews',ascending=False)[:5]

"""## 5. Which beer has the best overall rating?"""

reviews.sort_values('review/overall',ascending=False)[:5]

"""## 6. The number of beers produced by each brewer"""

brewers = pd.DataFrame(df.groupby('beer/brewerId')[['beer/beerId']].count())
brewers = brewers.sort_values(by='beer/beerId', ascending=False)
plt.figure(figsize=(20,10))
brewers.head(30).plot(kind='bar')
plt.title("Distribution of Beers By Brewer ID")
plt.show()

"""## 6. The Distribution of Overall Ratings of Beers:"""

ratings_column = df['review/overall']

# Plotting the histogram
plt.hist(ratings_column, bins=[0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5], edgecolor='black', width=0.5)

# Adding labels and title
plt.xlabel('Ratings')
plt.ylabel('Frequency')
plt.title('Distribution of Ratings')

# Display the plot
plt.show()

"""## 11. Temporal trends: How do beer reviews and ratings distribute over time?

need to add why are we not choosing years
"""

month_avg = pd.DataFrame(df.groupby('mon')[['review/overall']].mean())
sns.lineplot(data=month_avg)

wday_avg = pd.DataFrame(df.groupby('wday')[['review/overall']].mean())
sns.lineplot(data=wday_avg)

hour_avg = pd.DataFrame(df.groupby('hour')[['review/overall']].mean())
sns.lineplot(data=hour_avg)

"""## 7. What is the relationship between the beer's characteristics (appearance, aroma, palate, taste) and its overall rating?"""

corr_df = df[['review/overall', 'review/aroma', 'review/palate', 'review/taste', 'review/appearance']]
corr = corr_df.corr()
corr.style.background_gradient(cmap='coolwarm')

"""## 8. Finding the Relative Standard Deviation of Overall Rating of Each Beer Style:

$\big($Relative Std of Beer Style's Overall Rating = $\frac{\text{Std. of Beer Style's Overall Rating}}{\text{Max. Standard Dev. (= 2.5)}}$$\big)$
"""

# Finding Standard Deviation of Overall Rating for each style of beer

beer_styles = sorted(list(set(df['beer/style'])))

def add_relative_style_std(df):
    mean_ratings = df.groupby('beer/style')['review/overall'].mean().rename('mean_rating')
    df = df.merge(mean_ratings, on='beer/style')
    df['squared_diff'] = (df['review/overall'] - df['mean_rating'])**2
    sum_squared_diffs = df.groupby('beer/style')['squared_diff'].sum().rename('sum_squared_diff')
    counts = df.groupby('beer/style').size().rename('count')
    relative_std = (sum_squared_diffs / counts).apply(lambda x: (x**0.5) / 2.5).rename('relative_style_std')
    df = df.merge(relative_std, on='beer/style')
    return df.drop(['mean_rating', 'squared_diff'], axis=1, inplace=False)

df = add_relative_style_std(df)

from matplotlib import pyplot as plt
z = set()
for _,row in df.iterrows():
    z.add((row['beer/style'], row['relative_style_std']))
z_sorted = sorted(list(z), key = lambda x: x[1])
xtop = []
ytop = []
xbottom = []
ybottom = []
for style, std in z_sorted[-5:]:
    xtop.append(style)
    ytop.append(std)
for style, std in z_sorted[:5]:
    xbottom.append(style)
    ybottom.append(std)
fig,ax = plt.subplots(ncols=2, figsize = (16, 5))
ax[0].bar(xtop, ytop)
ax[0].set_xlabel('style')
ax[0].set_ylabel('standard deviation')
ax[0].tick_params(labelrotation=45)
ax[0].set_title('Top 5')
ax[1].bar(xbottom, ybottom)
ax[1].set_xlabel('style')
ax[1].set_ylabel('standard deviation')
ax[1].tick_params(labelrotation=45)
ax[1].set_title('Bottom 5')
plt.show()

"""## 8. Sentiment analysis of beer reviews: How many are positive, negative, or neutral?"""

# from textblob import TextBlob

# def get_sentiment(text):
#     blob = TextBlob(text)
#     return blob.polarity

# def get_sentiment_label(text):
#     blob = TextBlob(text)
#     if blob.polarity > 0:
#         result = 'positive'
#     elif blob.polarity < 0:
#         result = 'negative'
#     else:
#         result = 'neutral'
#     return result

# df['sentiment'] = df['review/text'].apply(get_sentiment)
# df['sentiment_label'] = df['review/text'].apply(get_sentiment_label)
# df[['review/text','sentiment','sentiment_label']]

# # How many positive and negative and neutral reviews?
# df['sentiment_label'].value_counts()

sns.lineplot(data=df,x='review/overall',y='sentiment')
plt.show()

sns.lineplot(data=df,x='review/overall',y='sentiment',hue='sentiment_label')

# genuine_bad = df[(df['review/overall'] < 4) & (df['sentiment_label'] == 'negative')]
# genuine_good = df[(df['review/overall'] >= 4) & (df['sentiment_label'] == 'positive')]
# old_df = df
# df = pd.concat([genuine_bad, genuine_good])

"""------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"""

dataset = df.to_dict('records')

#!wget 'https://drive.google.com/file/d/1-3rqM1bqWrVcGERohlVF790WwEi_xmlQ/download?usp=drive_link'
!gdown 'https://drive.google.com/uc?id=1i0yXMzUzkn7zE8hLlyGSSYGcam3EWQAO'

df = pd.read_csv("/content/drive/MyDrive/CSE 258-Assignment2/data100k_newest.csv", sep = ',')

dataset = df.to_dict(orient='records')

df.to_csv('/content/drive/MyDrive/data_258/data100k_new.csv', index=False)
df.to_json('/content/drive/MyDrive/data_258/data100k_new.json', orient='records')

print(dataset[0])

"""# Predictions:"""

beerstyles = list(set([d['beer/style'] for d in dataset]))
num_beerstyles = len(beerstyles)

maxlen_rev =  max([len(d['review/text']) for d in dataset])

beerstyle_map = defaultdict()

index = 0
for style in beerstyles:
    beerstyle_map[style] = index
    index += 1

dataset[0]

from sklearn.model_selection import train_test_split

def feature4(datum):
    weekd_vec, month_vec, beerstyle_vec = [0]*6, [0]*11, [0]*num_beerstyles

    if(beerstyle_map[datum['beer/style']] > 0):
        beerstyle_vec[beerstyle_map[datum['beer/style']] - 1] = 1

    feature = [1, *beerstyle_vec, datum['beer/ABV'], datum['review/palate'], datum['review/taste'], datum['review/aroma'], datum['review/appearance'], len(datum['review/text'])/maxlen_rev]
    feat_list = np.array(feature, dtype=object)

    return feat_list

X = [feature4(d) for d in dataset]
y = [int(d['review/overall'] >= 4) for d in dataset]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)

def BER(y_test, y_pred):
  TP = 0
  TN = 0
  FP = 0

  for i in range(len(y_pred)):
      if(y_pred[i] and y_test[i]):
          TP += 1
      elif(not(y_pred[i]) and not(y_test[i])):
          TN += 1
      elif(y_pred[i] and not(y_test[i])):
          FP += 1

  FN = len(y_pred) - (TP + TN + FP)

  BER = 0.5 * ((FN/(FN + TP)) + (FP/(FP + TN)))

  print(BER)

import xgboost as xgb

model = xgb.XGBClassifier()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

BER(np.array(y_test), y_pred)

import lightgbm as lgb

model = lgb.LGBMClassifier()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

BER(np.array(y_test), y_pred)

from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

base_classifier = LogisticRegression()
model = AdaBoostClassifier(base_classifier, n_estimators=100)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

BER(np.array(y_test), y_pred)

#Logistic Regression

model = LogisticRegression(max_iter = 1000)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

BER(np.array(y_test), y_pred)

#Random Forest Classifier

model = RandomForestClassifier()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

BER(np.array(y_test), y_pred)

#SVM Classifier

from sklearn import svm

model = svm.SVC()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

BER(np.array(y_test), y_pred)

"""### Regression"""

y = [d['review/overall'] for d in dataset]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

#Linear Regression

from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(X_train, y_train)

ypred = model.predict(X_test)

mse = (1/len(X_test)) * sum([(a - b)**2 for a,b in zip(ypred,y_test)])

print(mse)

!pip install mord

#Ordinal Regression

from mord import OrdinalRidge

model = OrdinalRidge().fit(np.array(X_train), np.array(y_train))

ypred = model.predict(np.array(X_test))

mse = (1/len(X_test)) * sum([(a - b)**2 for a,b in zip(ypred,y_test)])

print(mse)

#RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor().fit(np.array(X_train), np.array(y_train))

ypred = model.predict(np.array(X_test))

mse = (1/len(X_test)) * sum([(a - b)**2 for a,b in zip(ypred,y_test)])

print(mse)

# MLP

from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPRegressor

mlp_regressor = MLPRegressor()

# Define the parameter grid to search over
param_grid = {
    'hidden_layer_sizes': [(50, 25), (100, 50), (50, 50, 25)],
    'activation': ['relu'],
    'max_iter': [500],
}

# Create GridSearchCV instance
grid_search = GridSearchCV(mlp_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters:", grid_search.best_params_)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Evaluate the best model
mse = (1/len(X_test)) * sum([(a - b)**2 for a,b in zip(ypred,y_test)])

print(mse)

pip install --upgrade scikit-learn

!gdown 'https://drive.google.com/uc?id=1Fd0l8JtAXOO1Ir8GPNcCJCRWSR7t1nsV'

f = open("data100k_sentiment.json")
dataset = []
for l in f:
    dataset.append(eval(l))
f.close()
dataset = dataset.squeeze(1)

dataset[0]



import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from transformers import TFBertModel, BertTokenizer
from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout
from tensorflow.keras.models import Model

# Load your dataset (replace this with your actual data loading process)
X_text, X_other_features = np.array([d['review/text'] for d in dataset]),np.array([feature4(d) for d in dataset])

# Split the data into training and testing sets
X_text_train, X_text_test, X_other_train, X_other_test, y_train, y_test = train_test_split(
    X_text, X_other_features, y, test_size=0.2, random_state=42, stratify=y
)

# Tokenize the text data using a BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
X_text_train_encoded = tokenizer(list(X_text_train), padding=True, truncation=True, return_tensors="tf", max_length=128)
X_text_test_encoded = tokenizer(list(X_text_test), padding=True, truncation=True, return_tensors="tf", max_length=128)

# Load the BERT model
bert_model = TFBertModel.from_pretrained("bert-base-uncased", trainable=False)

# Create the text input layer
text_input = Input(shape=(128,), dtype=tf.int32, name="text_input")

# Generate text embeddings using BERT
text_embeddings = bert_model(text_input)[0][:, 0, :]

# Create the other features input layer
other_input = Input(shape=(X_other_train.shape[1],), name="other_input")

# Concatenate text embeddings with other features
concatenated_features = Concatenate()([text_embeddings, other_input])

# Add dropout for regularization
concatenated_features = Dropout(0.5)(concatenated_features)

# Create the output layer
output_layer = Dense(1, activation="sigmoid", name="output")(concatenated_features)

# Create the model
model = Model(inputs=[text_input, other_input], outputs=output_layer)

# Compile the model
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Standardize the other features
scaler = StandardScaler()
X_other_train_scaled = scaler.fit_transform(X_other_train)
X_other_test_scaled = scaler.transform(X_other_test)

# Train the model
model.fit(
    [X_text_train_encoded["input_ids"], X_other_train_scaled],
    y_train,
    epochs=5,
    batch_size=32,
    validation_split=0.2
)

# Evaluate the model
y_pred = (model.predict([X_text_test_encoded["input_ids"], X_other_test_scaled]) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

model = Model(inputs=[text_input, other_input], outputs=output_layer)

# Compile the model
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

model.fit(
    [X_text_train_encoded["input_ids"], X_other_train_scaled],
    np.array(y_train),
    epochs=5,
    batch_size=128,
    validation_split=0.2
)

# Evaluate the model
y_pred = (model.predict([X_text_test_encoded["input_ids"], X_other_test_scaled]) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print("Classification Report:")
print(classification_report(np.array(y_test), y_pred))

# ber = 0.5 * (conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0]) +
#              conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1]))

# print("Binary Error Rate (BER):", ber)





import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import balanced_accuracy_score

# Assuming 'dataset' is a list of dictionaries as you've shown above.
df = pd.DataFrame(dataset)

# Normalize the review features by dividing by 5
review_features = ['review/palate', 'review/taste', 'review/aroma', 'review/appearance']
df[review_features] = df[review_features].apply(lambda x: x / 5)

# Add a new feature for the length of the review text
df['review/length'] = df['review/text'].apply(len)

# Update the features to include the new feature and normalized review features
features = ['beer/style', 'beer/ABV', *review_features, 'review/length']

# Define target variable
target = 'review/overall'

# Convert 'review/overall' into a binary classification problem
df[target] = (df[target] >= 4).astype(int)

# Create the feature matrix X and target vector y
X = df[features]
y = df[target]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Preprocessing for numerical data: scaling
numeric_features = ['beer/ABV', 'review/palate', 'review/taste', 'review/aroma', 'review/appearance', 'review/length']
numeric_transformer = StandardScaler()

# Preprocessing for categorical data: one-hot encoding
categorical_features = ['beer/style']
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a MLPClassifier with a specified architecture
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=1000, random_state=42)

# Create a pipeline that combines the preprocessor with the MLP model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', mlp)])

from sklearn.model_selection import GridSearchCV

# Define a parameter grid to search over
param_grid = {
    'classifier__hidden_layer_sizes': [(50, 25), (100, 50), (150, 100)],
    'classifier__activation': ['relu'],
    'classifier__solver': ['adam'],
    'classifier__alpha': [0.001, 0.01],  # L2 penalty (regularization term) parameter
    'classifier__learning_rate_init': [0.01, 0.1],  # The initial learning rate
}

# Create a GridSearchCV object
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='balanced_accuracy', verbose=1)

# Execute the grid search
grid_search.fit(X_train, y_train)

# Best parameters found
print(grid_search.best_params_)

# Use the best estimator to predict and evaluate the model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Recalculate BER
ber = balanced_accuracy_score(y_test, y_pred)
print(f'Improved Balanced Error Rate (BER): {ber}')

# Train the MLP model
pipeline.fit(X_train, y_train)

# Predict and evaluate the model
y_pred = pipeline.predict(X_test)

# Calculate Balanced Error Rate (BER)
ber = balanced_accuracy_score(y_test, y_pred)

print(f'Balanced Error Rate (BER): {ber}')